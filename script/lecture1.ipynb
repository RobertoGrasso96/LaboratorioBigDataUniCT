{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio di Big Data - a.a. 2022/2023\n",
    "\n",
    "Materiale a cura di Roberto Grasso. \n",
    "\n",
    "roberto.grasso@phd.unict.it\n",
    "\n",
    "# Lecture 1\n",
    "\n",
    "In questa lezione studieremo le basi di Spark.\n",
    "\n",
    "## Da MapReduce a Spark\n",
    "Perché passare da MapReduce a Spark?\n",
    "\n",
    "1. Prestazioni migliori: Spark è noto per avere prestazioni migliori rispetto a MapReduce grazie alla sua architettura in-memory e alla sua capacità di sfruttare meglio le risorse hardware.\n",
    "2. Maggiore disponibilità di API: Spark offre un maggior numero di API rispetto a MapReduce. Questo rende possibile scrivere codice in diversi linguaggi di programmazione, tra cui Java, Scala, Python e R.\n",
    "3. Ampia gamma di librerie: Spark offre un'ampia gamma di librerie integrate, tra cui Spark SQL per l'elaborazione di dati strutturati, MLlib per il machine learning e GraphX per l'elaborazione di grafi.\n",
    "4. Facilità di utilizzo: Spark ha un'interfaccia utente più semplice rispetto a MapReduce, che lo rende più facile da utilizzare.\n",
    "5. Supporto della comunità: Spark ha una grande e attiva comunità di sviluppatori che fornisce supporto e risorse per gli utenti.\n",
    "\n",
    "\n",
    "## Il costrutto chiave: Resilient Distributed Dataset (RDD)\n",
    "\n",
    "Il __Resilient Distributed Dataset__ (RDD) è una struttura di dati fondamentale in Spark.\n",
    "Un RDD rappresenta una collezione *immutabile* e *distribuita* di oggetti che possono essere processati in parallelo attraverso un cluster.\n",
    "\n",
    "Con il termine *immutabile* si indende che, una volta creato, l'RDD non può essere modificato ma solo trasformato in un nuovo RDD tramite delle operazioni che vedremo a breve. Questo significa che ogni operazione su un RDD produce un nuovo RDD senza influenzare l'originale. L'immutabilità consente una migliore gestione della concorrenza e una maggiore efficienza in alcune operazioni (poiché le parti di un RDD possono essere elaborate in parallelo senza dover gestire la sincronizzazione tra i thread).\n",
    "\n",
    "Ci sono due modi principali per creare un RDD: leggere dati da una fonte esterna come Hadoop Distributed File System (HDFS) o da un database, o trasformare un RDD esistente attraverso le operazioni map, filter, reduce, aggregate, ecc..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operazioni base sugli RDD\n",
    "\n",
    "Le operazioni sugli RDD si possono suddividere in trasformazioni e azioni.\n",
    "* Trasformazioni: sono operazioni che consentono di costruire un RDD attraverso delle operazioni deterministiche su altri RDD (`map`, `filter`, `join`, `union`, `intersection`, `distinct`, ecc.). Queste operazioni sono _lazy_: non si calcola nulla fino a quando non è esplicitamente richiesto da un'azione.\n",
    "* Azioni: sono operazioni che servono per restituire valori o esportare dati (`count`, `collect`, `reduce`, `save`, ecc.).  Queste azioni forzano le operazioni di trasformazione e restituiscono un nuovo RDD.\n",
    "\n",
    "Cominciamo creando una sessione Spark e caricando un semplice file di testo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import hash\n",
    "from pyspark import StorageLevel\n",
    "import random\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, FloatType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/02 09:16:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Sessione Spark\n",
    "spark = SparkSession.builder.appName(\"LabBigData23\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Caricamento dati da un file di testo\n",
    "rdd = sc.textFile(\"../data/file1.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `collect()` ritorna gli elementi di un RDD come un array.\n",
    "Va usata con cautela perché nelle applicazioni reali solitamente si lavora con dataset molto grandi. Di conseguenza questa operazione può essere molto onerosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1,John,Doe,25,1000,IT', '2,Jane,Smith,32,1500,IT', '3,Bob,Johnson,45,2500,Sales', '4,Alice,Jones,28,1200,Sales', '5,Mark,Williams,38,2000,Sales']\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `map()` applica un'operazione ad ogni elemento di un RDD e ne restituisce uno nuovo che contiene il risultato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,JOHN,DOE,25,1000,IT',\n",
       " '2,JANE,SMITH,32,1500,IT',\n",
       " '3,BOB,JOHNSON,45,2500,SALES',\n",
       " '4,ALICE,JONES,28,1200,SALES',\n",
       " '5,MARK,WILLIAMS,38,2000,SALES']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upper\n",
    "rdd.map(lambda x: x.upper()).collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `filter()` consente di selezionare un sottoinsieme degli elementi di un RDD in base ad una condizione booleana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,John,Doe,25,1000,IT',\n",
       " '2,Jane,Smith,32,1500,IT',\n",
       " '3,Bob,Johnson,45,2500,Sales',\n",
       " '4,Alice,Jones,28,1200,Sales']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Righe che contengono \"J\"\n",
    "rdd.filter(lambda x: \"J\" in x).collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `flatMap()` applica una funzione ad ogni elemento dell'RDD, fa il _flattening_ del risultato e restituisce un nuovo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'John',\n",
       " 'Doe',\n",
       " '25',\n",
       " '1000',\n",
       " 'IT',\n",
       " '2',\n",
       " 'Jane',\n",
       " 'Smith',\n",
       " '32',\n",
       " '1500',\n",
       " 'IT',\n",
       " '3',\n",
       " 'Bob',\n",
       " 'Johnson',\n",
       " '45',\n",
       " '2500',\n",
       " 'Sales',\n",
       " '4',\n",
       " 'Alice',\n",
       " 'Jones',\n",
       " '28',\n",
       " '1200',\n",
       " 'Sales',\n",
       " '5',\n",
       " 'Mark',\n",
       " 'Williams',\n",
       " '38',\n",
       " '2000',\n",
       " 'Sales']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separiamo con la virgola e compattiamo in un unico array\n",
    "rdd.flatMap(lambda x: x.split(',')).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `mapValues()` funziona solo su RDD organizzati in coppie `(key, value)`. Applica un'operazione solo al valore di ciascuna coppia `(key, value)` dell'RDD e restituisce il risultato delll'operazione "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('IT', 'Doe'), ('IT', 'Smith'), ('Sales', 'Johnson'), ('Sales', 'Jones'), ('Sales', 'Williams')]\n",
      "[('IT', 'DOE'), ('IT', 'SMITH'), ('Sales', 'JOHNSON'), ('Sales', 'JONES'), ('Sales', 'WILLIAMS')]\n"
     ]
    }
   ],
   "source": [
    "# Per prima cosa creiamo un RDD con coppie (key, value).\n",
    "mv_rdd = rdd.map(lambda x: x.split(',')).map(lambda x: (x[5], x[2]))\n",
    "\n",
    "# Vediamo il contenuto del nuovo RDD.\n",
    "print(mv_rdd.collect())\n",
    "\n",
    "# Applichiamo la funzione mapValues per trasformare i valori.\n",
    "print(mv_rdd.mapValues(lambda x: x.upper()).collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di procedere con le altre funzioni, organizziamo i nostri dati in modo da avere delle tuple e non delle semplici stringhe. Possiamo farlo usando la funzione `map()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 'John', 'Doe', '25', '1000', 'IT'),\n",
       " ('2', 'Jane', 'Smith', '32', '1500', 'IT'),\n",
       " ('3', 'Bob', 'Johnson', '45', '2500', 'Sales'),\n",
       " ('4', 'Alice', 'Jones', '28', '1200', 'Sales'),\n",
       " ('5', 'Mark', 'Williams', '38', '2000', 'Sales')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splittiamo le stringhe con la virgola\n",
    "new_rdd = rdd.map(lambda x: tuple(x.split(',')))\n",
    "\n",
    "# Vediamo il contenuto del nuovo RDD\n",
    "new_rdd.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `filter()` viene utilizzata per filtrare gli elementi di un RDD di PySpark in base a una condizione. Restituisce un nuovo RDD con gli elementi che soddisfano la condizione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 'Bob', 'Johnson', '45', '2500', 'Sales'),\n",
       " ('5', 'Mark', 'Williams', '38', '2000', 'Sales')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtriamo gli impiegati in base allo stipendio\n",
    "\n",
    "# Questa riga di codice restituisce un errore perché di default tutti gli elementi sono stringhe\n",
    "# new_rdd.filter(lambda x: x[4] > 1800).collect()\n",
    "\n",
    "# Quindi, prima di filtrare, dobbiamo convertire lo stipendio in un numero\n",
    "new_rdd.filter(lambda x: float(x[4]) > 1800).collect()\n",
    "\n",
    "# Notate che la conversione in float è servita solo per il filtraggio (le modifiche non sono state apportate all'RDD)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `groupByKey()`, come la funzione `mapValues()`, funziona su coppie `(key, value)`. Come si può intuire dal nome, si limita a raggruppare gli elementi in base alla chiave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('IT', ('Doe', 1000.0)), ('IT', ('Smith', 1500.0)), ('Sales', ('Johnson', 2500.0)), ('Sales', ('Jones', 1200.0)), ('Sales', ('Williams', 2000.0))]\n",
      "[('Sales', [('Johnson', 2500.0), ('Jones', 1200.0), ('Williams', 2000.0)]), ('IT', [('Doe', 1000.0), ('Smith', 1500.0)])]\n"
     ]
    }
   ],
   "source": [
    "# Creiamo un nuovo RDD organizzato in coppie (key, value)\n",
    "kv_rdd = new_rdd.map(lambda x: (x[5], (x[2], float(x[4]))))\n",
    "\n",
    "# Abbiamo creato un nuovo RDD in cui la chiave è il nome del reparto e il valore è una tupla contenente il cognome dell'impiegato e lo stipendio\n",
    "print(kv_rdd.collect())\n",
    "\n",
    "# Raggruppiamo i dati per reparto\n",
    "print(kv_rdd.groupByKey().map(lambda x: (x[0], list(x[1]))).collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `reduceByKey()` restituisce un nuovo RDD con i valori ridotti per ogni chiave. Prende come argomento una funzione di riduzione che accetta due valori e ne restituisce uno solo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sales', ('', 5700.0)), ('IT', ('', 2500.0))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sommiamo gli stipendi per reparto\n",
    "kv_rdd.reduceByKey(lambda x, y: (\"\", x[1] + y[1])).collect()\n",
    "\n",
    "# La funzione di riduzione, in questo caso, prende in input due tuple e restituisce una tupla con il primo elemento vuoto e il secondo elemento dato dalla somma degli stipendi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `reduce()` è più generica. Infatti, non raggrupperà gli elementi dell'RDD in base alla chiave, ma applicherà la funzione di riduzione a tutti gli elementi in maniera indiscriminata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', ('', 8200.0))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sommiamo gli stipendi di tutti i dipendenti, indipendentemente dal reparto\n",
    "kv_rdd.reduce(lambda x, y: (\"\", (\"\", x[1][1] + y[1][1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La join è un'operazione che consente di unire due RDD in base alle chiavi in comune. Ci sono diverse tiplogie di join.\n",
    "* Inner join: restituisce solo le righe in cui il valore della chiave è presente in entrambi gli RDD.\n",
    "* Left Outer Join: restituisce tutte le righe del primo RDD (quello di sinistra) e le righe corrispondenti del secondo RDD (quello di destra). Se una riga del primo RDD non ha una corrispondenza nel secondo RDD, viene restituito un valore `NULL` per le colonne del secondo RDD.\n",
    "* Right Outer Join: restituisce tutte le righe del secondo RDD (quello di destra) e le righe corrispondenti del primo RDD (quello di sinistra). Se una riga del secondo RDD non ha una corrispondenza nel primo RDD, viene restituito un valore `NULL` per le colonne del primo RDD.\n",
    "* Full Outer Join: restituisce tutte le righe dei due RDD. Se una riga del primo o del secondo RDD non ha una corrispondenza nell'RDD opposto, viene restituito un valore `NULL` per le colonne mancanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sales', (('Johnson', 2500.0), 'Sales Department')),\n",
       " ('Sales', (('Jones', 1200.0), 'Sales Department')),\n",
       " ('Sales', (('Williams', 2000.0), 'Sales Department')),\n",
       " ('IT', (('Doe', 1000.0), 'Information Technology Department')),\n",
       " ('IT', (('Smith', 1500.0), 'Information Technology Department'))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vediamo un esempio di inner join\n",
    "\n",
    "# Creiamo un nuovo RDD in cui la chiave è la sigla del reparto e il valore è il nome del reparto\n",
    "data =  [('IT', ('Information Technology Department')),\n",
    "        ('Sales', ('Sales Department')),\n",
    "        ('HR', ('Human Resources Department'))]\n",
    "\n",
    "kv_rdd2 = sc.parallelize(data)\n",
    "\n",
    "# Associamo a ciascun dipendente, il nome del reparto\n",
    "kv_rdd.join(kv_rdd2).collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sales', (('Johnson', 2500.0), 'Sales Department')),\n",
       " ('Sales', (('Jones', 1200.0), 'Sales Department')),\n",
       " ('Sales', (('Williams', 2000.0), 'Sales Department')),\n",
       " ('IT', (('Doe', 1000.0), 'Information Technology Department')),\n",
       " ('IT', (('Smith', 1500.0), 'Information Technology Department')),\n",
       " ('HR', (None, 'Human Resources Department'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vediamo un esempio di right outer join\n",
    "kv_rdd.rightOuterJoin(kv_rdd2).collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `union()` viene utilizzata per combinare due RDD al fine di crearne uno nuovo che contiene tutti gli elementi di entrambi gli RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IT', ('Doe', 1000.0)),\n",
       " ('IT', ('Smith', 1500.0)),\n",
       " ('IT', ('Springsteen', 2000)),\n",
       " ('IT', ('Dylan', 1800)),\n",
       " ('Sales', ('Johnson', 2500.0)),\n",
       " ('Sales', ('Jones', 1200.0)),\n",
       " ('Sales', ('Williams', 2000.0))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creiamo un nuovo RDD\n",
    "data =  [('IT', ('Springsteen', 2000)), \n",
    "        ('IT', ('Dylan', 1800))]\n",
    "\n",
    "kv_rdd3 = sc.parallelize(data)\n",
    "\n",
    "# Union\n",
    "kv_rdd.union(kv_rdd3).sortByKey().collect()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per concludere, vediamo un esempio in cui contiamo il numero di parole contenute all'interno di un  file di testo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11388"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leggiamo il file di testo\n",
    "lorem_ipsum_rdd = sc.textFile(\"../data/file2.txt\")\n",
    "\n",
    "# Ciascuna riga viene vista come una stringa. Per contare il numero di parole, dobbiamo prima dividere le stringhe in parole\n",
    "lorem_ipsum_rdd.flatMap(lambda x: x.split(' ')).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con una sola riga di codice (escludendo la lettura da file) siamo riusciti a contare il numero di parole.\n",
    "\n",
    "Lo stesso possiamo fare per il conteggio del numero di caratteri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75803"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contiamo il numero di caratteri (compresi gli spazi)\n",
    "lorem_ipsum_rdd.map(lambda x: len(x)).reduce(lambda x, y: x + y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistenza e Caching\n",
    "\n",
    "La __persistenza__ è una tecnica di ottimizzazione usata da Spark per migliorare le performance di accesso ai dati. \n",
    "Si può scegliere di memorizzare i dati in memoria, su disco o di utilizzare un approccio _ibrido_.\n",
    "\n",
    "Le possibili opzioni sono:\n",
    "* `MEMORY_ONLY`: mantiene l'RDD in memoria (come un oggetto Java deserialized).\n",
    "* `MEMORY_ONLY_SER`: mantiene l'RDD in memoria (come un oggetto Java serialized).\n",
    "* `MEMORY_AND_DISK`: mantiene l'RDD in memoria (come un oggetto Java deserialized) e salva l'eccesso su disco.\n",
    "* `MEMORY_AND_DISK_SER`: mantiene l'RDD in memoria (come un oggetto Java serialized) e salva l'eccesso su disco.\n",
    "* `DISK_ONLY`: memorizza l'RDD su disco.\n",
    "\n",
    "Per cambiare la persistenza di un RDD, basta chiamare la funzione `persist()`. Per rilasciare la memoria occupata da un RDD con persistenza, basta chiamare la funzione `unpersist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Serialized 1x Replicated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[58] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creiamo un RDD e impostiamo il livello di persistenza\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "# Verifichiamo il livello di persistenza\n",
    "print(rdd.getStorageLevel())\n",
    "\n",
    "# Liberiamo la memoria\n",
    "rdd.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `cache()` equivale alla `persistence(MEMORY_ONLY)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Serialized 1x Replicated\n",
      "Memory Serialized 1x Replicated\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[58] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impostiamo il livello di persistenza a MEMORY_ONLY\n",
    "rdd.persist(StorageLevel.MEMORY_ONLY)\n",
    "\n",
    "# Verifichiamo il livello di persistenza\n",
    "print(rdd.getStorageLevel())\n",
    "\n",
    "# Liberiamo la memoria\n",
    "rdd.unpersist()\n",
    "\n",
    "# Chiamiamo la funzione cache()\n",
    "rdd.cache()\n",
    "\n",
    "# Verifichiamo il livello di persistenza\n",
    "print(rdd.getStorageLevel())\n",
    "\n",
    "# Liberiamo la memoria\n",
    "rdd.unpersist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame\n",
    "\n",
    "I __DataFrame__ in Spark sono delle strutture dati immutabili e distribuite. Utilizzano un approccio tabellare in cui ogni colonna ha un nome e un tipo di dato, e tutti i dati all'interno di una colonna devono essere dello stesso tipo. Sono concettualmente equivalenti alle tabelle di un database relazionale e ai dataframe di R/Python.\n",
    "\n",
    "I DataFrame possono essere creati a partire da diverse fonti di dati, come file CSV, file di testo, database esterni, RDD, ecc.\n",
    "\n",
    "I DataFrame possono essere manipolati utilizzando una varietà di operazioni. Supportano sia operazioni SQL-like che operazioni come map, filter, groupBy, ecc. Tutte le operazioni di trasformazione producono un nuovo DataFrame, mantenendo l'originale immutato.\n",
    "\n",
    "La creazione di un dataframe avviene con la funzione `createDataframe()`.  Si posono costruire a partire da: liste di liste, dizionari, `pyspark.sql.Row`, ecc...\n",
    "\n",
    "Si può decidere di specificare uno _schema_ in cui sono definiti i tipi delle colonne del dataframe (se omesso viene inferito in automatico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: float (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- first_name: string (nullable = false)\n",
      " |-- last_name: string (nullable = false)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creiamo un DF senza specificare lo scherma\n",
    "df = spark.createDataFrame([\n",
    "    Row(id=1, first_name=\"John\", last_name=\"Doe\", age=25, salary=1000.0, department=\"IT\"),\n",
    "    Row(id=2, first_name=\"Jane\", last_name=\"Smith\", age=32, salary=1500.0, department=\"IT\"),\n",
    "    Row(id=3, first_name=\"Bob\", last_name=\"Johnson\", age=45, salary=2500.0, department=\"Sales\"),\n",
    "    Row(id=4, first_name=\"Alice\", last_name=\"Jones\", age=28, salary=1200.0, department=\"Sales\"),\n",
    "    Row(id=5, first_name=\"Mike\", last_name=\"Williams\", age=35, salary=1800.0, department=\"Sales\"),\n",
    "])\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Creiamo lo stesso DF specificando lo schema\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"John\", \"Doe\", 25, 1000.0, \"IT\"),\n",
    "    (2, \"Jane\", \"Smith\", 32, 1500.0, \"IT\"),\n",
    "    (3, \"Bob\", \"Johnson\", 45, 2500.0, \"Sales\"),\n",
    "    (4, \"Alice\", \"Jones\", 28, 1200.0, \"Sales\"),\n",
    "    (5, \"Mark\", \"Williams\", 38, 2000.0, \"Sales\")\n",
    "], schema='id int, first_name string, last_name string, age int, salary float, department string')\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# Un altro modo per definire uno schema \n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# crea il DataFrame\n",
    "data = [\n",
    "    (1, \"John\", \"Doe\", 25, 1000.0, \"IT\"),\n",
    "    (2, \"Jane\", \"Smith\", 32, 1500.0, \"IT\"),\n",
    "    (3, \"Bob\", \"Johnson\", 45, 2500.0, \"Sales\"),\n",
    "    (4, \"Alice\", \"Jones\", 28, 1200.0, \"Sales\"),\n",
    "    (5, \"Mark\", \"Williams\", 38, 2000.0, \"Sales\")\n",
    "]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vediamo alcune funzioni utili.\n",
    "\n",
    "Le prime `k` righe di un DataFrame si possono visualizzare con la funzione `show(k)`. È inoltre possibile visualizzarle in verticale qualora dovessero essere troppo lunghe da visualizzare in orizzontale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+------+----------+\n",
      "| id|first_name|last_name|age|salary|department|\n",
      "+---+----------+---------+---+------+----------+\n",
      "|  1|      John|      Doe| 25|1000.0|        IT|\n",
      "|  2|      Jane|    Smith| 32|1500.0|        IT|\n",
      "+---+----------+---------+---+------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0------------\n",
      " id         | 1      \n",
      " first_name | John   \n",
      " last_name  | Doe    \n",
      " age        | 25     \n",
      " salary     | 1000.0 \n",
      " department | IT     \n",
      "-RECORD 1------------\n",
      " id         | 2      \n",
      " first_name | Jane   \n",
      " last_name  | Smith  \n",
      " age        | 32     \n",
      " salary     | 1500.0 \n",
      " department | IT     \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizziamo le prime due righe\n",
    "df.show(2)\n",
    "\n",
    "# Visualizziamo le prime due righe in verticale\n",
    "df.show(2, vertical=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È possibile ottenere i nomi delle colonne di un dataFrame tramite `colunms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'first_name', 'last_name', 'age', 'salary', 'department']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La funzione `select()` consente di selezionare alcune colonne del DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "|      Jane|    Smith|\n",
      "|       Bob|  Johnson|\n",
      "|     Alice|    Jones|\n",
      "|      Mark| Williams|\n",
      "+----------+---------+\n",
      "\n",
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "|      Jane|    Smith|\n",
      "|       Bob|  Johnson|\n",
      "|     Alice|    Jones|\n",
      "|      Mark| Williams|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Selezioniamo le colonne \"first_name\" e \"last_name\"\n",
    "df.select(\"first_name\", \"last_name\").show()\n",
    "\n",
    "# Un altro modo per selezionare le stesse colonne\n",
    "df.select(df.first_name, df.last_name).show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tramite la funzione `describe()` possiamo ottenere alcune informazioni sul DataFrame (utili soprattutto per le colonne numeriche)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+\n",
      "|summary|              age|           salary|\n",
      "+-------+-----------------+-----------------+\n",
      "|  count|                5|                5|\n",
      "|   mean|             33.6|           1640.0|\n",
      "| stddev|8.018728078691781|610.7372593840988|\n",
      "|    min|               25|           1000.0|\n",
      "|    max|               45|           2500.0|\n",
      "+-------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"age\", \"salary\").describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come per gli RDD, anche nei DataFrame è presente la funzione `collect()`. È tuttavia preferibile usare funzioni come `tail()` o `take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(id=1, first_name='John', last_name='Doe', age=25, salary=1000.0, department='IT'), Row(id=2, first_name='Jane', last_name='Smith', age=32, salary=1500.0, department='IT'), Row(id=3, first_name='Bob', last_name='Johnson', age=45, salary=2500.0, department='Sales'), Row(id=4, first_name='Alice', last_name='Jones', age=28, salary=1200.0, department='Sales'), Row(id=5, first_name='Mark', last_name='Williams', age=38, salary=2000.0, department='Sales')]\n",
      "[Row(id=1, first_name='John', last_name='Doe', age=25, salary=1000.0, department='IT')]\n",
      "[Row(id=5, first_name='Mark', last_name='Williams', age=38, salary=2000.0, department='Sales')]\n"
     ]
    }
   ],
   "source": [
    "# Collezioniamo tutte le righe del dataframe in una lista e stampiamole\n",
    "print(df.collect())\n",
    "\n",
    "# Collezioniamo la prima riga del dataframe e stampiamola\n",
    "print(df.take(1))\n",
    "\n",
    "# Collezioniamo l'ultima riga del dataframe e stampiamola\n",
    "print(df.tail(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'altra funzione da usare con cautela è `toPandas()` che, come si capisce dal nome, colleziona tutti gli elementi di un DataFrame Spark e li riversa su un DataFrame di Pandas (questo può causare problemi di memoria in caso di DataFrame molto grandi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>25</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Smith</td>\n",
       "      <td>32</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bob</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>45</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Alice</td>\n",
       "      <td>Jones</td>\n",
       "      <td>28</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>Sales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Mark</td>\n",
       "      <td>Williams</td>\n",
       "      <td>38</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Sales</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id first_name last_name  age  salary department\n",
       "0   1       John       Doe   25  1000.0         IT\n",
       "1   2       Jane     Smith   32  1500.0         IT\n",
       "2   3        Bob   Johnson   45  2500.0      Sales\n",
       "3   4      Alice     Jones   28  1200.0      Sales\n",
       "4   5       Mark  Williams   38  2000.0      Sales"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo già visto negli esempi precedenti, la funzione `select()` prende in input uno o più oggetti di tipo `Column` e restituisce un nuovo DataFrame. Infatti, per selezionare una colonna specifica di un DataFrame, non è sufficiente usare un oggetto di tipo `Column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'salary'>\n"
     ]
    }
   ],
   "source": [
    "print(df.salary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuttavia, gli oggetti di tipo `Column` vengono usati da moltissime funzioni. Ad esempio, per assegnare una nuova colonna al DataFrame usando la funzione `withColumn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+------+----------+-------------+\n",
      "| id|first_name|last_name|age|salary|department|annual_salary|\n",
      "+---+----------+---------+---+------+----------+-------------+\n",
      "|  1|      John|      Doe| 25|1000.0|        IT|      12000.0|\n",
      "|  2|      Jane|    Smith| 32|1500.0|        IT|      18000.0|\n",
      "|  3|       Bob|  Johnson| 45|2500.0|     Sales|      30000.0|\n",
      "|  4|     Alice|    Jones| 28|1200.0|     Sales|      14400.0|\n",
      "|  5|      Mark| Williams| 38|2000.0|     Sales|      24000.0|\n",
      "+---+----------+---------+---+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creiamo un nuovo DataFrame aggiungendo una nuova colonna (semplicemente moltiplicando per 12 la colonna \"salary\")\n",
    "df.withColumn(\"annual_salary\", df.salary * 12).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come per gli RDD, anche con i DataFrame si possono usare le funzioni `filter()`, `groupBy()`, ecc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|avg(salary)|\n",
      "+----------+-----------+\n",
      "|        IT|     1250.0|\n",
      "|     Sales|     1900.0|\n",
      "+----------+-----------+\n",
      "\n",
      "+---+----------+---------+---+------+----------+\n",
      "| id|first_name|last_name|age|salary|department|\n",
      "+---+----------+---------+---+------+----------+\n",
      "|  3|       Bob|  Johnson| 45|2500.0|     Sales|\n",
      "|  5|      Mark| Williams| 38|2000.0|     Sales|\n",
      "+---+----------+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Raggruppiamo per dipartimento e calcoliamo lo stipendio medio\n",
    "df.groupBy(df.department).agg({\"salary\": \"avg\"}).show()\n",
    "\n",
    "# Selezioniamo gli impiegati che guadagnano più di 1800\n",
    "df.filter(df.salary > 1800).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È possibile applicare delle `UDF` (User Defined Functions) alle colonne dei DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+------+----------+------------+\n",
      "| id|first_name|last_name|age|salary|department|salary_level|\n",
      "+---+----------+---------+---+------+----------+------------+\n",
      "|  1|      John|      Doe| 25|1000.0|        IT|         LOW|\n",
      "|  2|      Jane|    Smith| 32|1500.0|        IT|      MEDIUM|\n",
      "|  3|       Bob|  Johnson| 45|2500.0|     Sales|        HIGH|\n",
      "|  4|     Alice|    Jones| 28|1200.0|     Sales|         LOW|\n",
      "|  5|      Mark| Williams| 38|2000.0|     Sales|        HIGH|\n",
      "+---+----------+---------+---+------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Esempio di UDF\n",
    "def salary_level(salary):\n",
    "    if salary <= 1200.0:\n",
    "        return \"LOW\"\n",
    "    elif salary > 1200.0 and salary < 1800.0:\n",
    "        return \"MEDIUM\"\n",
    "    else:\n",
    "        return \"HIGH\"\n",
    "\n",
    "    \n",
    "salary_level_udf = udf(salary_level, StringType())\n",
    "\n",
    "# Aggiungiamo una colonna al DataFrame\n",
    "df.withColumn(\"salary_level\", salary_level_udf(df.salary)).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si possono anche utilizzare le `Pandas UDF`, che consentono di lavorare direttamente sulle `Series` e sui `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|department|annual_avg_salary|\n",
      "+----------+-----------------+\n",
      "|        IT|          15000.0|\n",
      "|     Sales|          22800.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definimao la Padas UDF\n",
    "@pandas_udf('float', PandasUDFType.GROUPED_AGG)\n",
    "def annual_avg_salary(s):\n",
    "    return s.mean() * 12\n",
    "\n",
    "# Calcoliamo lo stipendio medio annuo per reparto\n",
    "df.groupby('department').agg(annual_avg_salary(df['salary']).alias('annual_avg_salary')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ovviamente i DataFrame possono anche essere letti da file (csv, ORC, Parquet, ecc...). Vediamo un esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+------+----------+\n",
      "| id|first_name|last_name|age|salary|department|\n",
      "+---+----------+---------+---+------+----------+\n",
      "|  1|      John|      Doe| 25|1000.0|        IT|\n",
      "|  2|      Jane|    Smith| 32|1500.0|        IT|\n",
      "|  3|       Bob|  Johnson| 45|2500.0|     Sales|\n",
      "|  4|     Alice|    Jones| 28|1200.0|     Sales|\n",
      "|  5|      Mark| Williams| 38|2000.0|     Sales|\n",
      "+---+----------+---------+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Leggiamo un DataFrame da CSV\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "csv_df = spark.read.csv(\"../data/file3.csv\", header=False, schema=schema)\n",
    "\n",
    "# Visualizziamo il contenuto\n",
    "csv_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifichiamolo e salviamolo su un altro file csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+---+------+----------+-------------+\n",
      "| id|first_name|last_name|age|salary|department|annual_salary|\n",
      "+---+----------+---------+---+------+----------+-------------+\n",
      "|  1|      John|      Doe| 25|1000.0|        IT|      12000.0|\n",
      "|  2|      Jane|    Smith| 32|1500.0|        IT|      18000.0|\n",
      "|  3|       Bob|  Johnson| 45|2500.0|     Sales|      30000.0|\n",
      "|  4|     Alice|    Jones| 28|1200.0|     Sales|      14400.0|\n",
      "|  5|      Mark| Williams| 38|2000.0|     Sales|      24000.0|\n",
      "+---+----------+---------+---+------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggiungiamo il salario annuo\n",
    "csv_df = csv_df.withColumn(\"annual_salary\", csv_df.salary * 12)\n",
    "\n",
    "# Visualizziamo il contenuto\n",
    "csv_df.show()\n",
    "\n",
    "# Puliamo la directory di output\n",
    "out = \"../data/file3_mod.csv\"\n",
    "\n",
    "def clean_output(out):\n",
    "    if os.path.exists(out):\n",
    "        if os.path.isdir(out):\n",
    "            shutil.rmtree(out)\n",
    "        else:\n",
    "            os.remove(out)\n",
    "\n",
    "clean_output(out)\n",
    "\n",
    "# Salviamolo su un altro file\n",
    "csv_df.write.csv(out)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notiamo che sono stati salvati diversi file. Approfondiremo meglio l'argomento quando tratteremo i metodi di partizionamento. Prima però vediamo un altro modo per manipolare i DataFrame.\n",
    "\n",
    "In Spark è possibile manipolare i DataFrame usando la sintassi SQL. Vediamo qualche esempio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|first_name|last_name|age|\n",
      "+----------+---------+---+\n",
      "|       Bob|  Johnson| 45|\n",
      "|      Mark| Williams| 38|\n",
      "+----------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registriamo il DataFrame come una tabella\n",
    "csv_df.createOrReplaceTempView(\"table_A\")\n",
    "\n",
    "# Visualizziamo nome, cognome ed età dei dipendenti che hanno un salario annuo maggiore di 200000\n",
    "spark.sql(\"SELECT first_name, last_name, age FROM table_A WHERE annual_salary > 20000.0\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inoltre, si possono definire ed applicare delle `UDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+------------+\n",
      "|first_name|last_name|age_plus_one|\n",
      "+----------+---------+------------+\n",
      "|      John|      Doe|          26|\n",
      "|      Jane|    Smith|          33|\n",
      "|       Bob|  Johnson|          46|\n",
      "|     Alice|    Jones|          29|\n",
      "|      Mark| Williams|          39|\n",
      "+----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definimao la Padas UDF\n",
    "@pandas_udf('integer')\n",
    "def add_one(s):\n",
    "    return s + 1\n",
    "\n",
    "# Registriamo la UDF\n",
    "spark.udf.register(\"add_one\", add_one)\n",
    "\n",
    "# Sommiamo 1 all'età di ciascun dipendente\n",
    "spark.sql(\"SELECT first_name, last_name, add_one(age) as age_plus_one FROM table_A\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metodi di partizionamento\n",
    "Esistono diversi metodi di partizionamento che possono essere utilizati a seconda delle esigenze. Non esiste un metodo migliore degli altri. Sarà necessario valutare il metodo da utilizzare caso per caso.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+-------+\n",
      "|Age|Department| ID|Student|\n",
      "+---+----------+---+-------+\n",
      "| 20|      Math|  0| 465341|\n",
      "| 21|   Physics|  1| 445955|\n",
      "| 22|  Medicine|  2| 571367|\n",
      "| 23|      Math|  3| 145105|\n",
      "| 20|   Physics|  4| 292369|\n",
      "| 21|  Medicine|  5| 803052|\n",
      "| 22|      Math|  6| 599024|\n",
      "| 23|   Physics|  7| 742335|\n",
      "+---+----------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definiamo una funzione per stampare le partizioni.\n",
    "def print_partitions(df):\n",
    "    print(f\"Number of partitions: {df.rdd.getNumPartitions()}\")\n",
    "    print(\"Content of each partition:\")\n",
    "    for i, part in enumerate(df.rdd.glom().collect()):\n",
    "        print(f\"Partition {i}: {part}\")\n",
    "\n",
    "# Creiamo un dataframe di esempio.\n",
    "departments = [\"Math\", \"Physics\", \"Medicine\"]\n",
    "ages = [20, 21, 22, 23]\n",
    "data = []\n",
    "for i in range(8):\n",
    "    data.append({\"ID\": i, \"Department\": departments[i % 3],  \"Student\": random.randint(100000, 999999), \"Age\": ages[i%len(ages)]})\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Visualizziamo i dati.\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hash Partitioning. I dati sono partizionati in base all'hash della chiave. In questo modo, gli elementi con la stessa chiave finiscono nella stessa partizione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+-------+----------+\n",
      "|Age|Department| ID|Student|      Hash|\n",
      "+---+----------+---+-------+----------+\n",
      "| 20|      Math|  0| 465341|1445171467|\n",
      "| 21|   Physics|  1| 445955|1130882753|\n",
      "| 22|  Medicine|  2| 571367|2024429527|\n",
      "| 23|      Math|  3| 145105|1445171467|\n",
      "| 20|   Physics|  4| 292369|1130882753|\n",
      "| 21|  Medicine|  5| 803052|2024429527|\n",
      "| 22|      Math|  6| 599024|1445171467|\n",
      "| 23|   Physics|  7| 742335|1130882753|\n",
      "+---+----------+---+-------+----------+\n",
      "\n",
      "Number of partitions: 3\n",
      "Content of each partition:\n",
      "Partition 0: []\n",
      "Partition 1: [Row(Age=20, Department='Math', ID=0, Student=465341, Hash=1445171467), Row(Age=22, Department='Medicine', ID=2, Student=571367, Hash=2024429527), Row(Age=23, Department='Math', ID=3, Student=145105, Hash=1445171467), Row(Age=21, Department='Medicine', ID=5, Student=803052, Hash=2024429527), Row(Age=22, Department='Math', ID=6, Student=599024, Hash=1445171467)]\n",
      "Partition 2: [Row(Age=21, Department='Physics', ID=1, Student=445955, Hash=1130882753), Row(Age=20, Department='Physics', ID=4, Student=292369, Hash=1130882753), Row(Age=23, Department='Physics', ID=7, Student=742335, Hash=1130882753)]\n"
     ]
    }
   ],
   "source": [
    "# Supponiamo di voler creare un dataframe con 3 partizioni e vogliamo che le partizioni siano create in base al campo Department.\n",
    "\n",
    "# Definiamo il numero di partizioni:\n",
    "numPartitions = 3\n",
    "\n",
    "# Per comodità, aggiungiamo una colonna con un hash del campo Department\n",
    "hash_df = df.withColumn(\"Hash\", hash(df[\"Department\"]))\n",
    "\n",
    "# Visualizziamo il dataframe\n",
    "hash_df.show()\n",
    "\n",
    "# Ci aspettiamo che i dati con lo stesso valore di Department siano nella stessa partizione\n",
    "hash_df = hash_df.repartition(numPartitions, \"Department\")\n",
    "\n",
    "# Visualizziamo le partizioni\n",
    "print_partitions(hash_df)\n",
    "\n",
    "# Puliamo la directory di output\n",
    "out = \"../data/file4.csv\"\n",
    "\n",
    "clean_output(out)\n",
    "\n",
    "# Salviamo il DataFrame\n",
    "hash_df.write.csv(out)\n",
    "\n",
    "# Notiamo come anche il file è stato opportunamente partizionato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+-------+----------+--------------------+\n",
      "|Age|Department| ID|Student|      Hash|Hash % numPartitions|\n",
      "+---+----------+---+-------+----------+--------------------+\n",
      "| 20|      Math|  0| 465341|1445171467|                   1|\n",
      "| 22|  Medicine|  2| 571367|2024429527|                   1|\n",
      "| 23|      Math|  3| 145105|1445171467|                   1|\n",
      "| 21|  Medicine|  5| 803052|2024429527|                   1|\n",
      "| 22|      Math|  6| 599024|1445171467|                   1|\n",
      "| 21|   Physics|  1| 445955|1130882753|                   2|\n",
      "| 20|   Physics|  4| 292369|1130882753|                   2|\n",
      "| 23|   Physics|  7| 742335|1130882753|                   2|\n",
      "+---+----------+---+-------+----------+--------------------+\n",
      "\n",
      "Number of partitions: 3\n",
      "Content of each partition:\n",
      "Partition 0: []\n",
      "Partition 1: [Row(Age=20, Department='Math', ID=0, Student=465341, Hash=1445171467, Hash % numPartitions=1), Row(Age=22, Department='Medicine', ID=2, Student=571367, Hash=2024429527, Hash % numPartitions=1), Row(Age=23, Department='Math', ID=3, Student=145105, Hash=1445171467, Hash % numPartitions=1), Row(Age=21, Department='Medicine', ID=5, Student=803052, Hash=2024429527, Hash % numPartitions=1), Row(Age=22, Department='Math', ID=6, Student=599024, Hash=1445171467, Hash % numPartitions=1)]\n",
      "Partition 2: [Row(Age=21, Department='Physics', ID=1, Student=445955, Hash=1130882753, Hash % numPartitions=2), Row(Age=20, Department='Physics', ID=4, Student=292369, Hash=1130882753, Hash % numPartitions=2), Row(Age=23, Department='Physics', ID=7, Student=742335, Hash=1130882753, Hash % numPartitions=2)]\n"
     ]
    }
   ],
   "source": [
    "# Effettivamente, non ci sono due partizioni distinte contenenti studenti che frequentano lo stesso dipartimento.\n",
    "# All'interno della stessa partizione, troviamo però studenti che frequentano dipartimenti diversi.\n",
    "# Questo perché gli oggetti vengono distribuiti nelle partizioni in base a: hash(key) % numPartitions.\n",
    "# Aggiungiamo un'altra colonna al dataframe.\n",
    "hash_df = hash_df.withColumn(\"Hash % numPartitions\", hash(hash_df[\"Department\"])%numPartitions)\n",
    "\n",
    "# Visualizziamo il dataframe.\n",
    "hash_df.show()\n",
    "\n",
    "# Visualizziamo le partizioni.\n",
    "print_partitions(hash_df)\n",
    "\n",
    "# E se volessimo partizioni contenenti esclusivamente studenti dello stesso dipartimento?\n",
    "# Per farlo, dobbiamo utilizzare un altro metodo di partizionamento: il Custom Partitioning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Custom partitioning. È possibile specificare una funzione di partizionamento personalizzata per soddisfare le esigenze specifiche dell'applicazione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|      _1|                  _2|\n",
      "+--------+--------------------+\n",
      "|    Math|{20, Math, 0, 465...|\n",
      "| Physics|{21, Physics, 1, ...|\n",
      "|Medicine|{22, Medicine, 2,...|\n",
      "|    Math|{23, Math, 3, 145...|\n",
      "| Physics|{20, Physics, 4, ...|\n",
      "|Medicine|{21, Medicine, 5,...|\n",
      "|    Math|{22, Math, 6, 599...|\n",
      "| Physics|{23, Physics, 7, ...|\n",
      "+--------+--------------------+\n",
      "\n",
      "Number of partitions: 3\n",
      "Content of each partition:\n",
      "Partition 0: [Row(_1='Math', _2=Row(Age=20, Department='Math', ID=0, Student=465341)), Row(_1='Math', _2=Row(Age=23, Department='Math', ID=3, Student=145105)), Row(_1='Math', _2=Row(Age=22, Department='Math', ID=6, Student=599024))]\n",
      "Partition 1: [Row(_1='Physics', _2=Row(Age=21, Department='Physics', ID=1, Student=445955)), Row(_1='Physics', _2=Row(Age=20, Department='Physics', ID=4, Student=292369)), Row(_1='Physics', _2=Row(Age=23, Department='Physics', ID=7, Student=742335))]\n",
      "Partition 2: [Row(_1='Medicine', _2=Row(Age=22, Department='Medicine', ID=2, Student=571367)), Row(_1='Medicine', _2=Row(Age=21, Department='Medicine', ID=5, Student=803052))]\n"
     ]
    }
   ],
   "source": [
    "# Per il Custom Partitioning dobbiamo usare la funzione partitionBy.\n",
    "# Per poter usare questa funzione, i dati devono essere nel formato (key, value). \n",
    "custom_partitioning = df.rdd \\\n",
    "    .map(lambda el: (el[\"Department\"], el)) \\\n",
    "    .toDF()\n",
    "\n",
    "# Visualizziamo il dataframe.\n",
    "custom_partitioning.show()\n",
    "\n",
    "# Il metodo partitionBy consente all'utente di usare delle funzioni \"custom\".\n",
    "# Questa funzione prende in input la key e restituisce un intero che indica la partizione in cui inserire l'elemento.\n",
    "# Nel nostro caso, per semplicità, la funzione restituirà l'indice del dipartimento all'interno della lista departments.\n",
    "def department_partitioning(k):\n",
    "    return departments.index(k)\n",
    "    \n",
    "\n",
    "# Applichiamo il partizionamento.\n",
    "custom_partitioning = custom_partitioning.rdd \\\n",
    "    .partitionBy(numPartitions, department_partitioning) \\\n",
    "    .toDF()\n",
    "\n",
    "# Visualizziamo le partizioni.\n",
    "print_partitions(custom_partitioning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Range partitioning. I dati sono ordinati in base al valore della chiave e poi divisi in partizioni di uguale intervallo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n",
      "Content of each partition:\n",
      "Partition 0: [Row(Age=20, Department='Math', ID=0, Student=465341), Row(Age=21, Department='Physics', ID=1, Student=445955), Row(Age=20, Department='Physics', ID=4, Student=292369), Row(Age=21, Department='Medicine', ID=5, Student=803052)]\n",
      "Partition 1: [Row(Age=22, Department='Medicine', ID=2, Student=571367), Row(Age=23, Department='Math', ID=3, Student=145105), Row(Age=22, Department='Math', ID=6, Student=599024), Row(Age=23, Department='Physics', ID=7, Student=742335)]\n"
     ]
    }
   ],
   "source": [
    "# Esempio di range partitioning\n",
    "range_partitioned = df.repartitionByRange(2, \"Age\")\n",
    "\n",
    "print_partitions(range_partitioned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
