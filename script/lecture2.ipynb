{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio di Big Data - a.a. 2022/2023\n",
    "\n",
    "Materiale a cura di Roberto Grasso. \n",
    "\n",
    "roberto.grasso@phd.unict.it\n",
    "\n",
    "# Lezione 2\n",
    "In questa lezione vedremo alcuni tra i più noti algoritmi messi a disposizione dalla libreria __MLlib__. \n",
    "\n",
    "__MLlib__ (Machine Learning Library) è una libreria di machine learning per Apache Spark. Essa fornisce una vasta gamma di algoritmi di machine learning scalabili, inclusi modelli di regressione, clustering, classificazione, raccomandazione e altro ancora. Questa libreria supporta anche varie funzionalità per la gestione dei dati, come la trasformazione dei dati in formato vettoriale, la normalizzazione dei dati, la rimozione dei valori mancanti e la selezione delle feature.\n",
    "\n",
    "\n",
    "\n",
    "Cominciamo importando tutto ciò che ci servirà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, FloatType\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, PCA, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.stat import Summarizer, KolmogorovSmirnovTest, Correlation\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator, RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import col, collect_list, array_distinct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creiamo una sessione Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 16:04:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Sessione Spark\n",
    "spark = SparkSession.builder.appName(\"LabBigData23\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eseguiremo buona parte degli algoritmi sul noto dataset __iris__. Esso contiene informazioni su tre varietà di iris (setosa, versicolor e virginica) Ognuna di esse è descritta da quattro attributi: lunghezza del sepalo, larghezza del sepalo, lunghezza del petalo e larghezza del petalo.\n",
    "\n",
    "Il dataset è composto da 150 osservazioni, dove ogni osservazione rappresenta una singola pianta di iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- sepal_length: float (nullable = true)\n",
      " |-- sepal_width: float (nullable = true)\n",
      " |-- petal_length: float (nullable = true)\n",
      " |-- petal_width: float (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      "\n",
      "+-------+------------------+-------------------+------------------+------------------+\n",
      "|summary|      sepal_length|        sepal_width|      petal_length|       petal_width|\n",
      "+-------+------------------+-------------------+------------------+------------------+\n",
      "|  count|               150|                150|               150|               150|\n",
      "|   mean| 5.843333326975505| 3.0540000025431313|3.7586666552225747| 1.198666658103466|\n",
      "| stddev|0.8280661128539085|0.43359431104332985|1.7644204144315179|0.7631607319020202|\n",
      "|    min|               4.3|                2.0|               1.0|               0.1|\n",
      "|    max|               7.9|                4.4|               6.9|               2.5|\n",
      "+-------+------------------+-------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definiamo lo schema del dataset\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"sepal_length\", FloatType(), False),\n",
    "    StructField(\"sepal_width\", FloatType(), False),\n",
    "    StructField(\"petal_length\", FloatType(), True),\n",
    "    StructField(\"petal_width\", FloatType(), True),\n",
    "    StructField(\"species\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Leggiamo il file csv\n",
    "iris = spark.read.csv(\"../data/file5.csv\", header=True, schema=schema)\n",
    "\n",
    "# Visualizziamo qualche informazione\n",
    "iris.printSchema()\n",
    "iris.describe([\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]).show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per comodità, facciamo un encoding della colonna `species`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-----------+------------+-----------+-----------+-----+\n",
      "| id|sepal_length|sepal_width|petal_length|petal_width|    species|label|\n",
      "+---+------------+-----------+------------+-----------+-----------+-----+\n",
      "|  1|         5.1|        3.5|         1.4|        0.2|Iris-setosa|    0|\n",
      "|  2|         4.9|        3.0|         1.4|        0.2|Iris-setosa|    0|\n",
      "|  3|         4.7|        3.2|         1.3|        0.2|Iris-setosa|    0|\n",
      "|  4|         4.6|        3.1|         1.5|        0.2|Iris-setosa|    0|\n",
      "|  5|         5.0|        3.6|         1.4|        0.2|Iris-setosa|    0|\n",
      "+---+------------+-----------+------------+-----------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Possiamo fare un encoding della colonna \"species\" usando uno StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"label\")\n",
    "\n",
    "# Aggiungiamo una nuova colonna al DatFrame\n",
    "iris = indexer.fit(iris).transform(iris)\n",
    "\n",
    "# Trasformiamo \"label\" in un intero\n",
    "iris = iris.withColumn(\"label\", iris[\"label\"].cast(IntegerType()))\n",
    "\n",
    "# Visualizziamo il DataFrame\n",
    "iris.show(5)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La maggior parte degli algoritmi della libreria __MLlib__ si aspettano gli input in un determinato formato. Le feature sono tipicamente rappresentate da un vettore (`pyspark.ml.linalg.Vectors`) mentre la variabile dipendente (sia essa continua o discreta) è un valore numerico. Abbiamo già fatto l'encoding della specie. Adesso creiamo per ciascuna osservazione un vettore con le feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+-----+\n",
      "|features                                                                    |label|\n",
      "+----------------------------------------------------------------------------+-----+\n",
      "|[5.099999904632568,3.5,1.399999976158142,0.20000000298023224]               |0    |\n",
      "|[4.900000095367432,3.0,1.399999976158142,0.20000000298023224]               |0    |\n",
      "|[4.699999809265137,3.200000047683716,1.2999999523162842,0.20000000298023224]|0    |\n",
      "+----------------------------------------------------------------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Poiché partiamo da un DatFrame, possiamo usare un VectorAssembler per creare un vettore di features\n",
    "vectorAssembler = VectorAssembler(inputCols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], outputCol = 'features')\n",
    "\n",
    "# Abbiamo specificato quali sono le feature che devono essere incluse. Trasformiamo il DataFrame\n",
    "viris = vectorAssembler.transform(iris)\n",
    "\n",
    "# Prendiamo solo le colonne che ci interessano (features e label)\n",
    "viris = viris.select(['features', 'label'])\n",
    "\n",
    "# Visualizziamo il nuovo DataFrame\n",
    "viris.show(3, truncate=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistica\n",
    "La libreria __MLlib__ offre gli strumenti classici per l'analisi statistica. In questa lezione vedremo come calcolare alcune statistiche del primo ordine, come effettuare un test di ipotesi e come calcolare alcuni tra i più noti indici di correlazione.\n",
    "\n",
    "### Statistiche del primo ordine\n",
    "Cominciamo con le statistiche del primo ordine. Possiamo velocemente calcolarle utilizzando un oggetto `Summarizer`. Basta specificare la colonna contenente il vettore delle feature e la statisica che si vuole calcolare. Vediamo subito qualche esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|mean                                                                         |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|[5.8433333269755074,3.0540000025431313,3.7586666552225756,1.1986666581034664]|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Media\n",
    "viris.select(Summarizer.mean(viris.features).alias(\"mean\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------+\n",
      "|variance                                                                     |\n",
      "+-----------------------------------------------------------------------------+\n",
      "|[0.685693487256982,0.18800402656913992,3.1131793988626892,0.5824143027172272]|\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Varianza\n",
    "viris.select(Summarizer.variance(viris.features).alias(\"variance\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+\n",
      "|max                                                        |\n",
      "+-----------------------------------------------------------+\n",
      "|[7.900000095367432,4.400000095367432,6.900000095367432,2.5]|\n",
      "+-----------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Massimo\n",
    "viris.select(Summarizer.max(viris.features).alias(\"max\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|min                                            |\n",
      "+-----------------------------------------------+\n",
      "|[4.300000190734863,2.0,1.0,0.10000000149011612]|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Minimo\n",
    "viris.select(Summarizer.min(viris.features).alias(\"min\")).show(truncate=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "È possibile associare un _peso_ a ciasuna osservazione e calcolare, ad esempio, la media pesata.\n",
    "Vediamo un esempio con un toy dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|weight|     features|\n",
      "+------+-------------+\n",
      "|   1.0|[1.0,1.0,1.0]|\n",
      "|   0.0|[1.0,2.0,3.0]|\n",
      "+------+-------------+\n",
      "\n",
      "+-------------+\n",
      "|mean         |\n",
      "+-------------+\n",
      "|[1.0,1.0,1.0]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Si può scegliere di aggiungere dei pesi alle osservazioni\n",
    "\n",
    "# Creiamo un nuovo DataFrame giocattolo con sole due colonne: peso e features\n",
    "example = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                            Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "\n",
    "# Visualizziamo il DataFrame\n",
    "example.show()\n",
    "\n",
    "# Calcoliamo la media \n",
    "example.select(Summarizer.mean(example.features, example.weight).alias(\"mean\")).show(truncate=False)\n",
    "\n",
    "# Notiamo che l'osservazione con peso 0 non viene considerata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|weight|     features|\n",
      "+------+-------------+\n",
      "|   1.0|[1.0,1.0,1.0]|\n",
      "|   0.5|[1.0,2.0,3.0]|\n",
      "|   0.8|[1.0,2.0,3.0]|\n",
      "+------+-------------+\n",
      "\n",
      "+-----------------------------------------+\n",
      "|mean                                     |\n",
      "+-----------------------------------------+\n",
      "|[1.0,1.565217391304348,2.130434782608696]|\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creiamo un altro DataFrame giocattolo con sole due colonne: peso e features\n",
    "example = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                            Row(weight=0.5, features=Vectors.dense(1.0, 2.0, 3.0)),\n",
    "                            Row(weight=0.8, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "\n",
    "# Visualizziamo il DataFrame\n",
    "example.show()\n",
    "\n",
    "# Calcoliamo la media\n",
    "example.select(Summarizer.mean(example.features, example.weight).alias(\"mean\")).show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test di ipotesi\n",
    "Vediamo come effettuare un test di ipotesi. In particolare, soffermiamoci sul test di __Kolmogorov-Smirnov__ a singolo campione. Questo test ci consente di confrontare una distribuzione empirica con una teorica (che può essere normale, uniforme o di Poisson).\n",
    "\n",
    "Il test confronta la distribuzione empirica dei dati con quella teorica e calcola la massima differenza assoluta tra le due distribuzioni. La differenza viene espressa come un valore di _statistica test_.\n",
    "\n",
    "Se il _p-value_ è inferiore a una soglia di significatività prefissata (tipicamente 0.05), allora si può rifiutare l'ipotesi nulla e concludere che il campione di dati non proviene dalla distribuzione teorica specificata. Altrimenti, se il _p-value_ è superiore alla soglia di significatività, si può accettare l'ipotesi nulla e concludere che non ci sono prove sufficienti per rifiutarla.\n",
    "\n",
    "In sintesi, il test di __Kolmogorov-Smirnov__ è un metodo per verificare se un campione di dati segue una distribuzione specifica ed è comunemente utilizzato per valutare la bontà di adattamento di un modello a una distribuzione dati specifica."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nel seguente esempio vogliamo confontare il nostro dataset (ogni feature) con la distribuzione normale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### sepal_length ###\n",
      "pValue: 0.0\n",
      "statistic: 0.999991460101879\n",
      "\n",
      " ### sepal_width ###\n",
      "pValue: 8.58291215877216e-12\n",
      "statistic: 0.979429887511395\n",
      "\n",
      " ### petal_length ###\n",
      "pValue: 1.43607348235264e-11\n",
      "statistic: 0.8765328405762314\n",
      "\n",
      " ### petal_width ###\n",
      "pValue: 6.474820679613913e-13\n",
      "statistic: 0.5398278378685344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definiamo una distribuzione teorica\n",
    "theoretical_distribution = \"norm\"\n",
    "\n",
    "# Definiamo le feature che vogliomo testare\n",
    "features = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# Facciamo il test per ciascuna feature\n",
    "for feature in features: \n",
    "    ks_test = KolmogorovSmirnovTest.test(iris, feature, theoretical_distribution)\n",
    "    \n",
    "    # Visualizziamo i risultati\n",
    "    results = ks_test.collect()\n",
    "    p_value = results[0][\"pValue\"]\n",
    "    statistic = results[0][\"statistic\"]\n",
    "    print(\" ### {} ###\\npValue: {}\\nstatistic: {}\\n\".format(feature, p_value, statistic))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlazione\n",
    "Per concludere la parte sulla statistica vediamo come calcolare la correlazione tra le feature del nostro dataset. La libreria MLlib permette di calcolare due tipi di correlazione: Pearson e Spearman.\n",
    "\n",
    "Cominciamo calcolando la __correlazione di Pearson__. L'indice di correlazione di Pearson è un numero compreso tra $-1$ e $1$ che indica se due variabili sono correlate, anticorrelate o non sono correlate. Valori prossimi a $-1$ indicano che le variabili sono anticorrelate (quando la prima variabile cresce, la seconda decresce e viceversa). Valori prossimi a $1$ indicano che le variabili sono correlate (quando la prima variabile cresce, anche la seconda cresce e viceversa). Infine, valori prossimi a $0$ indicano che le variabili non sono correlate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 16:04:44 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/03 16:04:44 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/05/03 16:04:44 WARN InstanceBuilder$JavaBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pearson(features)                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0                   -0.10936928622571358  0.871754165666988     0.8179536373532048   \\n-0.10936928622571358  1.0                   -0.42051611476913947  -0.3565441008826461  \\n0.871754165666988     -0.42051611476913947  1.0                   0.9627570943307204   \\n0.8179536373532048    -0.3565441008826461   0.9627570943307204    1.0                  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo la correlazione\n",
    "pearson_corr = Correlation.corr(viris, \"features\", method=\"pearson\")\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "pearson_corr.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cerchiamo di visualizzare meglio i risultati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Pearson ###\n",
      "       0     1     2     3\n",
      "0  1.00 -0.11  0.87  0.82\n",
      "1 -0.11  1.00 -0.42 -0.36\n",
      "2  0.87 -0.42  1.00  0.96\n",
      "3  0.82 -0.36  0.96  1.00\n"
     ]
    }
   ],
   "source": [
    "# Salviamo i risultati in un DataFrame di pandas\n",
    "pearson_df = pd.DataFrame(pearson_corr.collect()[0][\"pearson(features)\"].toArray())\n",
    "\n",
    "# Arrotondiamo alla seconda cifra decimale\n",
    "pearson_df = pearson_df.round(2)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "print(\"### Pearson ###\\n\", pearson_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passiamo adesso alla __correlazione di Spearman__. La correlazione di Pearson funziona quando ci sono pochi outlier, quando le variabili sono distribuite in maniera normale e quando sono legate da una relazione lineare. L'indice di correlazione per ranghi di Spearman risolve questi problemi ordinando tutti i valori e sostituendo ad essi il loro rango (la posizione che essi occupano nella lista ordinata) e calcolando l'indice di Pearson sui ranghi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Spearman ###\n",
      "       0     1     2     3\n",
      "0  1.00 -0.16  0.88  0.83\n",
      "1 -0.16  1.00 -0.30 -0.28\n",
      "2  0.88 -0.30  1.00  0.94\n",
      "3  0.83 -0.28  0.94  1.00\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo la correlazione\n",
    "spearman_corr = Correlation.corr(viris, \"features\", method=\"spearman\")\n",
    "\n",
    "# Salviamo i risultati in un DataFrame di pandas\n",
    "spearmann_df = pd.DataFrame(spearman_corr.collect()[0][\"spearman(features)\"].toArray())\n",
    "\n",
    "# Arrotondiamo alla seconda cifra decimale\n",
    "spearmann_df = spearmann_df.round(2)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "print(\"### Spearman ###\\n\", spearmann_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "Sfruttiamo gli ultimi esempi per introdurre la __PCA__. La __Principal Component Analysis__ (PCA) è una tecnica di riduzione della dimensionalità utilizzata per identificare i pattern nascosti nei dati, rivelando le relazioni tra le variabili. L'obiettivo della PCA è quello di trasformare un insieme di variabili correlate in un nuovo insieme di variabili non correlate, note come componenti principali. In pratica, la PCA identifica gli assi di maggiore varianza nei dati e proietta i dati su questi assi per ottenere i nuovi set di variabili non correlate.\n",
    "\n",
    "Nel nostro caso non abbiamo molte variabili (anche se alcune di esse sono correlate). Infatti, il seguente esempio serve solo a mostrarvi come eseguire la PCA con MLlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 16:04:56 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "23/05/03 16:04:56 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "+----------------------------------------+\n",
      "|pca_features                            |\n",
      "+----------------------------------------+\n",
      "|[-2.827135891184163,-5.641330980579563] |\n",
      "|[-2.7959524725854408,-5.145166936416606]|\n",
      "|[-2.621523420100777,-5.1773780328276695]|\n",
      "|[-2.7649058506572253,-5.003599278626428]|\n",
      "|[-2.7827500765369875,-5.64864822663768] |\n",
      "+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definiamo: il numero di componenti, la colonna con le feature e il nome della colonna dove verranno salvate le nuove feature.\n",
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "\n",
    "# Aggiungiamo una colonna al DataFrame. Questa colonna conterrà le nuove feature\n",
    "model = pca.fit(viris)\n",
    "transformed = model.transform(viris).select(col(\"pca_features\"))\n",
    "\n",
    "# Visualizziamo l'output\n",
    "transformed.show(5, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci aspettiamo che le nuove feature non siano correlate. Calcoliamo la correlazione usando entrambi i metodi visti in precedenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Pearson ###\n",
      "      0    1\n",
      "0  1.0  0.0\n",
      "1  0.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo la correlazione\n",
    "pearson_corr = Correlation.corr(transformed, \"pca_features\", method=\"pearson\")\n",
    "\n",
    "# Salviamo i risultati in un DataFrame di pandas\n",
    "pearson_df = pd.DataFrame(pearson_corr.collect()[0][\"pearson(pca_features)\"].toArray())\n",
    "\n",
    "# Arrotondiamo alla seconda cifra decimale\n",
    "pearson_df = pearson_df.round(2)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "print(\"### Pearson ###\\n\", pearson_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Spearman ###\n",
      "       0     1\n",
      "0  1.00  0.14\n",
      "1  0.14  1.00\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo la correlazione\n",
    "spearman_corr = Correlation.corr(transformed, \"pca_features\",  method=\"spearman\")\n",
    "\n",
    "# Salviamo i risultati in un DataFrame di pandas\n",
    "spearmann_df = pd.DataFrame(spearman_corr.collect()[0][\"spearman(pca_features)\"].toArray())\n",
    "\n",
    "# Arrotondiamo alla seconda cifra decimale\n",
    "spearmann_df = spearmann_df.round(2)\n",
    "\n",
    "# Visualizziamo i risultati\n",
    "print(\"### Spearman ###\\n\", spearmann_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come previsto, le nuove feature non sono correlate tra loro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing\n",
    "Il __Locality Sensitive Hashing__ è una tecnica utilizzata per la ricerca di item simili in dataset di grandi dimensioni. Si effettua un mapping degli item appartenenti ad uno spazio ad alta dimensionalità in un altro spazio a dimensionalità più bassa. L'idea è che, tramite hashing, elementi _simili_ verranno assegnati allo stesso bucket. Questo consente di velocizzare notevolmente il tempo richiesto per la ricerca di item simili, pur mantenendo un tasso di errore basso.\n",
    "\n",
    "Ci sono diverse varianti dell'LSH. Una di queste è nota come __Bucketed Random Projection (BRP)__. \n",
    "\n",
    "Il processo di BRP si svolge in tre fasi:\n",
    "\n",
    "1. _Generazione delle matrici di proiezione random_. Supponiamo di avere dei dati in uno spazio tridimensionale e di volerli mappare in uno spazio bidimensionale. In questo caso verrà creata una matrice di proizione di dimensione $3 \\times 2$. Ciascuna entry della matrice sarà randomica. Ad esempio:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "10 & 1 & -2\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "1 & -1  \\\\\n",
    "1 & 1 \\\\\n",
    "-1 & -1\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "13 & -7\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "2. _Proiezione dei dati_. I vettori originali vengono moltiplicati per la matrice di proiezione. In questo modo vengono mappati in uno spazio a dimensionalità più bassa.\n",
    "\n",
    "3. _Bucketing_. I vettori proiettati vengono raggruppati in bucket.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Vediamo un esempio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=DenseVector([4.9, 3.0, 1.4, 0.2]), label=0)\n",
      "[Row(features=DenseVector([4.9, 3.0, 1.4, 0.2]), label=0, hashes=[DenseVector([0.0]), DenseVector([2.0]), DenseVector([-1.0])], distCol=0.0), Row(features=DenseVector([4.8, 3.0, 1.4, 0.1]), label=0, hashes=[DenseVector([0.0]), DenseVector([2.0]), DenseVector([-1.0])], distCol=0.1414212898560397), Row(features=DenseVector([4.8, 3.0, 1.4, 0.3]), label=0, hashes=[DenseVector([0.0]), DenseVector([2.0]), DenseVector([-1.0])], distCol=0.1414212951243984), Row(features=DenseVector([4.9, 3.1, 1.5, 0.1]), label=0, hashes=[DenseVector([0.0]), DenseVector([2.0]), DenseVector([-1.0])], distCol=0.1732050403219206), Row(features=DenseVector([4.9, 3.1, 1.5, 0.1]), label=0, hashes=[DenseVector([0.0]), DenseVector([2.0]), DenseVector([-1.0])], distCol=0.1732050403219206)]\n"
     ]
    }
   ],
   "source": [
    "# Facciamo un Bucketed Random Projection LSH con 3 tabelle hash\n",
    "brp_lsh = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=3, bucketLength=2.0)\n",
    "brp_lsh.setSeed(10)\n",
    "\n",
    "# Applichiamo il modello al nostro dataset\n",
    "model = brp_lsh.fit(viris)\n",
    "transformed = model.transform(viris)\n",
    "\n",
    "# Prendiamo un item a caso \n",
    "random_row = viris.sample(False, 0.5, seed=100).take(1)[0]\n",
    "\n",
    "# Visualizziamolo\n",
    "print(random_row)\n",
    "\n",
    "# Cerchiamo i suoi 5 vicini più prossimi\n",
    "print(model.approxNearestNeighbors(transformed, random_row.features, 5).collect())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Il __clustering__ è una tecnica di analisi dei dati utilizzata per identificare gruppi omogenei all'interno di un dataset. L'obiettivo del clustering è quello di organizzare gli elementi di un dataset in un certo numero di gruppi, detti cluster, tali che gli oggetti all'interno di ciascun cluster sono più simili tra loro rispetto agli oggetti in altri cluster. \n",
    "\n",
    "### K-Means\n",
    "\n",
    "Vediamo un esempio con uno degli algortimi di clustering più famosi: il __K-Means__. Il K-Means è un algoritmo di clustering semi-supervisionato che cerca di dividere un insieme di dati in K cluster (K è un  parametro dell'algoritmo). L'obiettivo è di minimizzare la somma delle distanze quadrate tra ogni punto e il suo centroide di appartenenza.\n",
    "\n",
    "L'algoritmo inizia con la scelta di K centroidi casuali all'interno del dataset. In seguito, per ogni punto viene calcolata la distanza euclidea tra esso e ogni centroide. Il punto viene quindi assegnato al cluster il cui centroide è più vicino. Una volta che tutti i punti sono stati assegnati ai cluster, i centroidi vengono aggiornati (media di tutti i punti appartenenti al cluster).\n",
    "\n",
    "Il processo di assegnazione dei punti ai cluster e di aggiornamento dei centroidi viene ripetuto fino a quando non si raggiunge un criterio di convergenza. Tipicamente, questo criterio è un limite massimo sul numero di iterazioni o un valore minimo sulla variazione della somma delle distanze quadrate tra i centroidi e i punti del cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------+-----+----------+\n",
      "|features                                                                    |label|prediction|\n",
      "+----------------------------------------------------------------------------+-----+----------+\n",
      "|[5.099999904632568,3.5,1.399999976158142,0.20000000298023224]               |0    |1         |\n",
      "|[4.900000095367432,3.0,1.399999976158142,0.20000000298023224]               |0    |1         |\n",
      "|[4.699999809265137,3.200000047683716,1.2999999523162842,0.20000000298023224]|0    |1         |\n",
      "+----------------------------------------------------------------------------+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+----------+-----+-----+\n",
      "|prediction|label|count|\n",
      "+----------+-----+-----+\n",
      "|         0|    1|   48|\n",
      "|         0|    2|   14|\n",
      "|         1|    0|   50|\n",
      "|         2|    1|    2|\n",
      "|         2|    2|   36|\n",
      "+----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eseguiamo l'algoritmo K-means con k = 3\n",
    "kmeans = KMeans(k=3, seed=1)\n",
    "model = kmeans.fit(viris.select(\"features\"))\n",
    "predictions = model.transform(viris.select(\"features\", \"label\"))\n",
    "\n",
    "# Vediamo i  risultati\n",
    "predictions.show(3, truncate=False)\n",
    "\n",
    "# Vediamo il numero di elementi di ciascuna classe all'interno dei 3 cluster\n",
    "class_counts = predictions.groupBy(\"prediction\", \"label\").count().orderBy(\"prediction\", \"label\")\n",
    "class_counts.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificazione\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La __classificazione__ è una tecnica di analisi dei dati supervisionata che consiste nell'assegnare degli oggetti a una o più categorie (classi) in base alle loro feature. Il processo di classificazione richiede un dataset, noto come training set, che verrà utilizzato per addestrare un modello e un altro dataset, noto come test set (diverso dal training set), che verrà utilizzato per testarlo.\n",
    "\n",
    "Dividiamo il nostro dataset in due parti: una per il training e una per il test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, test_data) = viris.randomSplit([0.8, 0.2], seed=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "Il primo metodo per la classificazione che vedremo è quello dei __Decision Tree__. Un albero decisionale è un modello di apprendimento supervisionato che può essere utilizzato sia per la classificazione che per la regressione. In pratica, un albero decisionale costruisce una serie di regole che vengono applicate in maniera gerarchica al fine di prevedere una classe (nel caso della classificazione) o un valore continuo (nel caso della regressione).\n",
    "\n",
    "Il decision tree è un modello di facile interpretazione e può essere visualizzato graficamente, rendendolo adatto a situazioni in cui è necessario spiegare le decisioni adottate dal modello a persone non esperte. Inoltre, può gestire sia variabili di input continue che discrete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|0    |0.0       |\n",
      "|0    |0.0       |\n",
      "|0    |0.0       |\n",
      "+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Accuracy = 0.90\n"
     ]
    }
   ],
   "source": [
    "# Addestriamo un Decision Tree\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "model = dt.fit(training_data)\n",
    "\n",
    "# Facciamo delle previsioni sul test set\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Visualizziamo qualche previsione\n",
    "predictions.select(\"label\", \"prediction\").show(3, truncate=False)\n",
    "\n",
    "# Calcoliamo e visualizziamo l'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy = {:.2f}'.format(accuracy))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Continuiamo con le __Random Forest__. Le Random Forest non fanno altro che allenare molti alberi decisionali (con una profondità limitata) in cui le soglie per lo splitting sono scelte a caso. Una volta che tutti gli alberi sono stati costruiti, la Random Forest combina le loro predizioni per determinare l'output finale. In caso di problemi di classificazione, la foresta restituisce la classe più frequente tra tutte le predizioni degli alberi. In caso di problemi di regressione, la foresta restituisce la media delle predizioni degli alberi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|0    |0.0       |\n",
      "|0    |0.0       |\n",
      "|0    |0.0       |\n",
      "+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "# Addestriamo una Random Forest\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "model = rf.fit(training_data)\n",
    "\n",
    "# Facciamo delle previsioni sul test set\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Visualizziamo qualche previsione\n",
    "predictions.select(\"label\", \"prediction\").show(3, truncate=False)\n",
    "\n",
    "# Calcoliamo e visualizziamo l'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy = {:.2f}'.format(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Concludiamo con la __Logistic Regression__. La regressione logistica è un algoritmo di apprendimento supervisionato utilizzato per la classificazione. L'algoritmo di regressione logistica utilizza la funzione logistica (o sigmoide) per stimare una  probabilità, che può assumere un valore compreso tra 0 e 1. Questa probabilità viene utilizzata per classificare i dati, utilizzando una soglia di decisione (ad esempio, se la probabilità è maggiore di 0,5, l'item verrà assegnato alla classe 1, altrimenti alla classe 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 16:05:24 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                                                                                                                                                                                |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |[17.99,10.38,122.8,1001.0,0.1184,0.2776,0.3001,0.1471,0.2419,0.07871,1.095,0.9053,8.589,153.4,0.006399,0.04904,0.05373,0.01587,0.03003,0.006193,25.38,17.33,184.6,2019.0,0.1622,0.6656,0.7119,0.2654,0.4601,0.1189,1.0] |\n",
      "|1.0  |[20.57,17.77,132.9,1326.0,0.08474,0.07864,0.0869,0.07017,0.1812,0.05667,0.5435,0.7339,3.398,74.08,0.005225,0.01308,0.0186,0.0134,0.01389,0.003532,24.99,23.41,158.8,1956.0,0.1238,0.1866,0.2416,0.186,0.275,0.08902,1.0]|\n",
      "|1.0  |[19.69,21.25,130.0,1203.0,0.1096,0.1599,0.1974,0.1279,0.2069,0.05999,0.7456,0.7869,4.585,94.03,0.00615,0.04006,0.03832,0.02058,0.0225,0.004571,23.57,25.53,152.5,1709.0,0.1444,0.4245,0.4504,0.243,0.3613,0.08758,1.0]  |\n",
      "|1.0  |[11.42,20.38,77.58,386.1,0.1425,0.2839,0.2414,0.1052,0.2597,0.09744,0.4956,1.156,3.445,27.23,0.00911,0.07458,0.05661,0.01867,0.05963,0.009208,14.91,26.5,98.87,567.7,0.2098,0.8663,0.6869,0.2575,0.6638,0.173,1.0]      |\n",
      "|1.0  |[20.29,14.34,135.1,1297.0,0.1003,0.1328,0.198,0.1043,0.1809,0.05883,0.7572,0.7813,5.438,94.44,0.01149,0.02461,0.05688,0.01885,0.01756,0.005115,22.54,16.67,152.2,1575.0,0.1374,0.205,0.4,0.1625,0.2364,0.07678,1.0]     |\n",
      "+-----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo il dataset\n",
    "breast_cancer_data = spark.read.csv(\"../data/file8.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Trasformiamo la colonna con la diagnosi in modo che contenga valori numerici (0 o 1)\n",
    "indexer = StringIndexer(inputCol=\"diagnosis\", outputCol=\"label\")\n",
    "breast_cancer_data = indexer.fit(breast_cancer_data).transform(breast_cancer_data)\n",
    "\n",
    "# Per ciascun elemento, raggruppiamo le feature in un vettore\n",
    "assembler = VectorAssembler(inputCols=breast_cancer_data.columns[2:], outputCol=\"features\")\n",
    "vbreast_cancer_data = assembler.transform(breast_cancer_data).select(col(\"label\"), col(\"features\"))\n",
    "\n",
    "# Visualizziamo qualche elemento\n",
    "vbreast_cancer_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anche in questo caso, suddividiamo il dataset in training e test\n",
    "(training_data, test_data) = vbreast_cancer_data.randomSplit([0.8, 0.2], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------------------------------+\n",
      "|label|prediction|probability                                |\n",
      "+-----+----------+-------------------------------------------+\n",
      "|0.0  |0.0       |[0.9999999997663465,2.336535409597218E-10] |\n",
      "|0.0  |0.0       |[0.9999999999880376,1.1962431045731137E-11]|\n",
      "|0.0  |0.0       |[0.9999999999993061,6.938893903907228E-13] |\n",
      "+-----+----------+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creiamo il modello per la regressione logistica\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Addestriamo il modello sul training set\n",
    "model = lr.fit(training_data)\n",
    "\n",
    "# Facciamo delle previsioni sul test set\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Visualizziamo qualche previsione\n",
    "predictions.select(\"label\", \"prediction\", \"probability\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo e visualizziamo l'AUC (Area Under the Curve)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print('AUC = {:.2f}'.format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo e visualizziamo l'accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print('Accuracy = {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo e visualizziamo la precision\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "precision = evaluator.evaluate(predictions)\n",
    "print('Precision = {:.2f}'.format(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo e visualizziamo la recall\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "recall = evaluator.evaluate(predictions)\n",
    "print('Recall = {:.2f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 1.00\n"
     ]
    }
   ],
   "source": [
    "# Calcoliamo e visualizziamo l'F1 score\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator.evaluate(predictions)\n",
    "print('F1 = {:.2f}'.format(f1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressione\n",
    "La __regressione__ è una tecnica che viene utilizzata per esplorare la relazione tra una o più variabili indipendenti e una variabile dipendente. Lo scopo della regressione è quello di trovare una funzione matematica che possa descrivere la relazione tra queste variabili.\n",
    "\n",
    "Carichiamo un dataset più adatto ad un algoritmo di regressione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|fixed_acidity|volatile_acidity|citric_acid|residual_sugar|chlorides|free_sulfur_dioxide|total_sulfur_dioxide|density|pH  |sulphates|alcohol|quality|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|7.4          |0.7             |0.0        |1.9           |0.076    |11.0               |34.0                |0.9978 |3.51|0.56     |9.4    |5      |\n",
      "|7.8          |0.88            |0.0        |2.6           |0.098    |25.0               |67.0                |0.9968 |3.2 |0.68     |9.8    |5      |\n",
      "|7.8          |0.76            |0.04       |2.3           |0.092    |15.0               |54.0                |0.997  |3.26|0.65     |9.8    |5      |\n",
      "|11.2         |0.28            |0.56       |1.9           |0.075    |17.0               |60.0                |0.998  |3.16|0.58     |9.8    |6      |\n",
      "|7.4          |0.7             |0.0        |1.9           |0.076    |11.0               |34.0                |0.9978 |3.51|0.56     |9.4    |5      |\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo il dataset\n",
    "wine_data = spark.read.csv(\"../data/file9.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Visualizziamo qualche elemento\n",
    "wine_data.show(5, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vogliamo provare a predire la qualità del vino a partire dalle feature che abbiamo a disposizione. Come visto in precedenza, dobbiamo raggruppare le feature di ciascun elemento in un vettore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------+\n",
      "|target|features                                                |\n",
      "+------+--------------------------------------------------------+\n",
      "|5     |[7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0.56,9.4]  |\n",
      "|5     |[7.8,0.88,0.0,2.6,0.098,25.0,67.0,0.9968,3.2,0.68,9.8]  |\n",
      "|5     |[7.8,0.76,0.04,2.3,0.092,15.0,54.0,0.997,3.26,0.65,9.8] |\n",
      "|6     |[11.2,0.28,0.56,1.9,0.075,17.0,60.0,0.998,3.16,0.58,9.8]|\n",
      "|5     |[7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0.56,9.4]  |\n",
      "+------+--------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Per ciascun elemento, raggruppiamo le feature in un vettore\n",
    "assembler = VectorAssembler(inputCols=wine_data.columns[:-1], outputCol=\"features\")\n",
    "vwine_data = assembler.transform(wine_data).select(col(\"quality\").alias(\"target\"), col(\"features\"))\n",
    "\n",
    "# Visualizziamo qualche elemento\n",
    "vwine_data.show(5, truncate=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividiamo il dataset in training e test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anche in questo caso, suddividiamo il dataset in training e test\n",
    "(training_data, test_data) = vwine_data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressione Lineare\n",
    "Esistono vari tipi di regressione. Uno tra questi è la __regressione lineare__. Questo tipo di regressione prevede l'andamento di una variabile dipendente a partire da uno o più variabili dipendenti assumendo che tale relazione sia lineare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.69\n"
     ]
    }
   ],
   "source": [
    "# Creiamo il modello per la regressione lineare\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"target\", regParam=0.1, elasticNetParam=0.5)\n",
    "\n",
    "# Addestriamo il modello sul training set\n",
    "model = lr.fit(training_data)\n",
    "\n",
    "# Facciamo delle previsioni sul test set\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Calcoliamo e visualizziamo il Root Mean Squared Error\n",
    "evaluator = RegressionEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('RMSE = {:.2f}'.format(rmse))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering\n",
    "\n",
    "Il __Collaborative Filtering__ è una tecnica utilizzata nei sistemi di raccomandazione per trovare le relazioni tra gli utenti e gli item (come ad esempio i film in una piattaforma di streaming) sulla base delle loro attività passate (come ad esempio i rating dati dagli utenti ai film).\n",
    "\n",
    "Esistono due tipi principali di Collaborative Filtering: item-based e user-based. Nel Collaborative Filtering user-based, le preferenze degli utenti sono utilizzate per trovare utenti simili, e queste similarità sono utilizzate per fare raccomandazioni sugli item. Nel Collaborative Filtering item-based, invece, le proprietà degli item sono utilizzate per trovare item simili, e queste similarità sono utilizzate per fare raccomandazioni agli utenti.\n",
    "\n",
    "Il Collaborative Filtering utilizza una matrice di associazione user-item, che contiene, ad esempio, i rating dati dagli utenti agli item. Tuttavia, questa matrice spesso contiene molte voci mancanti (poiché gli utenti non hanno valutato tutti gli item). Il Collaborative Filtering utilizza il machine learning per trovare le relazioni tra gli utenti e gli item. In MLlib, l'algoritmo proposto per il Collaborative Filtering è l'__ALS__ (Alternating Least Square). \n",
    "\n",
    "Cominciamo caricando un dataset adatto a questo genere di task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|movieId|rating|userId|\n",
      "+-------+------+------+\n",
      "|2      |3.0   |0     |\n",
      "|3      |1.0   |0     |\n",
      "|5      |2.0   |0     |\n",
      "|9      |4.0   |0     |\n",
      "|11     |1.0   |0     |\n",
      "+-------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|            rating|\n",
      "+-------+------------------+\n",
      "|  count|              1501|\n",
      "|   mean|1.7741505662891406|\n",
      "| stddev| 1.187276166124803|\n",
      "|    min|               1.0|\n",
      "|    max|               5.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo il dataset\n",
    "recc_data = spark.read.csv(\"../data/file6.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# Stampiamo qualche elemento\n",
    "recc_data.show(5, truncate=False)\n",
    "\n",
    "# Vediamo qualche informazione sui rating\n",
    "recc_data.select(\"rating\").describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividiamo il dataset in training e test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anche in questo caso, suddividiamo il dataset in training e test\n",
    "(training_data, test_data) = recc_data.randomSplit([0.8, 0.2], seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 16:06:38 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                    |\n",
      "+------+-----------------------------------------------------------------------------------+\n",
      "|20    |[{22, 3.8885803}, {94, 3.202127}, {77, 3.123834}, {88, 3.0215375}, {51, 2.954056}] |\n",
      "|10    |[{92, 3.484316}, {2, 3.3816395}, {40, 2.8593414}, {25, 2.7985897}, {49, 2.7787867}]|\n",
      "|0     |[{92, 3.2650762}, {2, 2.8636913}, {25, 2.670202}, {12, 2.4387572}, {49, 2.2559018}]|\n",
      "+------+-----------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-----------------------------------------------------------------------------------+\n",
      "|movieId|recommendations                                                                    |\n",
      "+-------+-----------------------------------------------------------------------------------+\n",
      "|20     |[{23, 2.299555}, {17, 2.2393234}, {5, 2.0602932}, {12, 1.9901173}, {29, 1.8804365}]|\n",
      "|40     |[{2, 3.55634}, {10, 2.8593414}, {28, 2.7848673}, {8, 2.314449}, {4, 2.2465422}]    |\n",
      "|10     |[{17, 3.5154326}, {23, 3.374879}, {5, 2.9533453}, {29, 2.6596353}, {12, 2.5603626}]|\n",
      "+-------+-----------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creiamo e addestriamo il modello per il collaborative filtering\n",
    "als = ALS(maxIter=5, regParam=0.1, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "model = als.fit(training_data)\n",
    "\n",
    "# Vediamo le migliori 5 valutazioni date da alcuni utenti\n",
    "userRecs = model.recommendForAllUsers(5)\n",
    "userRecs.show(3, truncate=False)\n",
    "\n",
    "# Veediamo quali sono le 5 migliori valutazioni date ad alcuni film\n",
    "movieRecs = model.recommendForAllItems(5)\n",
    "movieRecs.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|userId|recommendations                                                                                                                                                        |\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|26    |[{22, 4.589559}, {7, 4.423425}, {88, 4.397339}, {23, 4.304868}, {24, 4.2404084}, {68, 3.6774354}, {36, 3.482273}, {30, 3.4684234}, {4, 3.3449988}, {51, 3.3309832}]    |\n",
      "|27    |[{30, 3.4716766}, {18, 3.412661}, {23, 2.9196398}, {32, 2.8294592}, {69, 2.7674785}, {7, 2.7615361}, {80, 2.761163}, {66, 2.6972814}, {83, 2.5878484}, {27, 2.5506296}]|\n",
      "|28    |[{92, 4.400953}, {81, 3.9778917}, {2, 3.8073535}, {12, 3.722697}, {49, 3.4508216}, {4, 3.1810398}, {82, 2.9086697}, {23, 2.8719664}, {25, 2.8508668}, {40, 2.7848673}] |\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|movieId|recommendations                                                                                                                                                       |\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|31     |[{12, 2.944835}, {25, 2.3405275}, {14, 2.2775137}, {8, 2.01658}, {15, 1.9309466}, {16, 1.7326337}, {17, 1.6923778}, {6, 1.5887675}, {7, 1.4900045}, {21, 1.4818431}]  |\n",
      "|85     |[{8, 4.1933403}, {16, 4.03093}, {14, 3.375039}, {24, 3.2506547}, {21, 3.0167868}, {7, 2.669526}, {1, 2.535785}, {4, 2.469184}, {20, 2.341584}, {6, 2.1948328}]        |\n",
      "|65     |[{23, 3.740344}, {17, 3.187525}, {12, 2.9333875}, {5, 2.8163452}, {29, 2.6930196}, {9, 2.6021795}, {11, 2.3600883}, {26, 1.7877891}, {27, 1.6836547}, {15, 1.6027185}]|\n",
      "+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Raccomandiamo 10 film ad alcuni utenti del test set\n",
    "users = test_data.select(als.getUserCol()).distinct().limit(3)\n",
    "users_rec = model.recommendForUserSubset(users, 10)\n",
    "\n",
    "# Visualizziamo le previsioni\n",
    "users_rec.show(truncate=False)\n",
    "\n",
    "# Vediamo quali sono gli utenti del test set a cui potrebbero piaceere alcuni film\n",
    "movies = training_data.select(als.getItemCol()).distinct().limit(3)\n",
    "movies_rec = model.recommendForItemSubset(movies, 10)\n",
    "\n",
    "movies_rec.show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequent Itemsets Mining & Association Rules\n",
    "Il __Frequent Itemset Mining__ è una tecnica di data mining utilizzata per trovare insiemi di item (articoli, prodotti, ecc.) molto frequenti all'interno di un dataset. Ad esempio, in un negozio, ci possono essere prodotti che vengono spesso acquistati insieme e l'identificazione di tali associazioni può essere utile per la strategia di marketing e per le raccomandazioni ai clienti.\n",
    "\n",
    "La tecnica si basa sulla scansione dei dati al fine di identificare gruppi di item che appaiono insieme nello stesso basket con una frequenza superiore a una certa soglia (chiamata _supporto_ minimo).\n",
    "\n",
    "Gli itemset frequenti vengono spesso presentati come regole if-then che prendono il nome di __Association Rules__. La forma di una regola di associazione è del tipo $I \\rightarrow J$, dove $I$ è un itemset e $J$ è un item. L'implicazione indica che se un basket contiene tutti gli item di $I$, allora è _verosimile_ che questo basket contenga anche $J$. Il concetto di _verosimiglianza_ viene formalizzato definendo la _confidence_ della regola (rapporto tra il supporto di $I \\cup \\{J\\}$ e il supporto di $I$).\n",
    "\n",
    "## FP-Growth\n",
    "La scoperta degli itemset frequenti può essere effettuata attraverso algoritmi come __FP-Growth__. Questo algoritmo codifica il dataset usando una struttura dati compatta che prende il nome di __FP-Tree__ ed estrae gli itemset frequenti direttamente da questa struttura. Un FP-Tree è una rappresentazione compatta dei basket del dataset. Si costruisce leggendo un basket alla volta e mappando ciascuno di questi in un percorso dell'FP_Tree. Diversi basket possono avere item in comune, quindi i loro percorsi possono sovrapporsi. Maggiore è il numero di sovrapposizioni, maggiore è la compressione ottenuta mediante la rappresentazione con l'FP-Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------------+\n",
      "|Member_number|Date      |itemDescription|\n",
      "+-------------+----------+---------------+\n",
      "|1808         |21-07-2015|tropical fruit |\n",
      "|2552         |05-01-2015|whole milk     |\n",
      "|2300         |19-09-2015|pip fruit      |\n",
      "+-------------+----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo il dataset\n",
    "baskets = spark.read.csv(\"../data/file7.csv\", inferSchema=True, header=True)\n",
    "\n",
    "# Stampiamo qualche elemento\n",
    "baskets.show(3, truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dobbiamo ri-organizzare il nostro dataset in modo da avere dei basket di prodotti. Per fare ciò, usiamo la funzione `groupBy` e `agg` di Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------------------------------------+\n",
      "|Member_number|Date      |items                                             |\n",
      "+-------------+----------+--------------------------------------------------+\n",
      "|1000         |15-03-2015|[sausage, whole milk, semi-finished bread, yogurt]|\n",
      "|1000         |24-06-2014|[whole milk, pastry, salty snack]                 |\n",
      "|1000         |24-07-2015|[canned beer, misc. beverages]                    |\n",
      "|1000         |25-11-2015|[sausage, hygiene articles]                       |\n",
      "|1000         |27-05-2015|[soda, pickled vegetables]                        |\n",
      "+-------------+----------+--------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Raggruppiamo in delle liste tutti gli item acquistati dalla stessa persona nello stesso giorno\n",
    "baskets = baskets.groupBy(\"Member_number\", \"Date\").agg(collect_list(\"itemDescription\").alias(\"items\"))\n",
    "\n",
    "# Rimuoviamo i duplicati da ciascuna lista\n",
    "baskets = baskets.withColumn(\"items\", array_distinct(\"items\"))\n",
    "\n",
    "# Stampiamo qualche elemento\n",
    "baskets.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|       items|freq|\n",
      "+------------+----+\n",
      "|      [beef]| 508|\n",
      "|[white wine]| 175|\n",
      "| [chocolate]| 353|\n",
      "+------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "+------------------+------------+-------------------+------------------+--------------------+\n",
      "|        antecedent|  consequent|         confidence|              lift|             support|\n",
      "+------------------+------------+-------------------+------------------+--------------------+\n",
      "|[other vegetables]|[rolls/buns]|0.08648056923918993|0.7861535586427697|0.010559379803515338|\n",
      "|[other vegetables]|[whole milk]|0.12151067323481117|0.7694304712706219|0.014836596939116486|\n",
      "|          [yogurt]|[whole milk]|0.12996108949416343|0.8229402378760761|0.011160863463209249|\n",
      "+------------------+------------+-------------------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creiamo e addestiamo il modello\n",
    "fp_growth = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.01)\n",
    "model = fp_growth.fit(baskets)\n",
    "\n",
    "# Visualizziamo alcuni tra gli itemset frequenti\n",
    "model.freqItemsets.show(3)\n",
    "\n",
    "# Visualizziamo alcune regole di associazione\n",
    "model.associationRules.show(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
